{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcampbell/miniconda3/envs/gopher/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/home/jcampbell/miniconda3/envs/gopher/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import spacy\n",
    "import scispacy\n",
    "from collections import Counter\n",
    "import pathlib\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from scispacy.linking import EntityLinker\n",
    "from negspacy.negation import Negex\n",
    "from negspacy.termsets import termset\n",
    "ts = termset(\"en_clinical\")\n",
    "\n",
    "# Intantiate the spacy program specifying the model you want to use\n",
    "nlp = spacy.load(\"en_core_sci_lg\")\n",
    "\n",
    "# Add the abbreviation pipe to the spacy pipeline.\n",
    "nlp.add_pipe(\"abbreviation_detector\")\n",
    "# Add the hpo entity linker pipe to the spacy pipeline.\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True,\n",
    "                                        \"linker_name\": \"umls\",\n",
    "                                        'threshold':0.85,\n",
    "                                        'max_entities_per_mention':1\n",
    "                                       })\n",
    "# add the sentiment analysis step\n",
    "nlp.add_pipe('spacytextblob')\n",
    "# add the negation detection\n",
    "nlp.add_pipe(\"negex\")\n",
    "ts.add_patterns({\n",
    "            \"pseudo_negations\": [],\n",
    "            \"termination\": [],\n",
    "            \"preceding_negations\": [\"was no\", \"were no\", \"were never\", \"showed no\", \"a negative\"],\n",
    "            \"following_negations\": [\"was negative\", \"remained negative\", \"was undetectable\"],\n",
    "        })\n",
    "\n",
    "# nlp.remove_pipe(\"scispacy_linker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1427\n"
     ]
    }
   ],
   "source": [
    "paed_path = '/home/jcampbell/paed_covid_case_reports/picu_control_set/output/retrieved_df/retrieved_df2.p'\n",
    "df = pickle.load(open(paed_path, 'rb'))   \n",
    "condition = [type(text) == str for text in df['content_text']]\n",
    "df=pd.DataFrame(df.loc[condition,:])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # abbreviations\n",
    "# abrvs = {'long_forms':[],'spans':[]}\n",
    "# for abrv in doc._.abbreviations:\n",
    "#     abrvs['long_forms'].append(abrv._.long_form)\n",
    "#     abrvs['spans'].append((abrv.start,abrv.end))\n",
    "    \n",
    "# Counter(abrvs['long_forms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # text = 'I had a really horrible day. It was the worst day ever! But every now and then I have a really good day that makes me happy.'\n",
    "# doc = nlp(df['content_text'][1])\n",
    "# for sent in doc.sents:\n",
    "#     polarity = sent._.polarity\n",
    "#     if polarity < -0.5:\n",
    "#         print(sent)\n",
    "#         print(sent._.assessments)\n",
    "# # ent = doc.ents[10]\n",
    "# # print(ent)\n",
    "# # # doc._.polarity      # Polarity: -0.125\n",
    "# # # doc._.subjectivity  # Sujectivity: 0.9\n",
    "# # # doc._.assessments   # Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\n",
    "# # ent._.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from negspacy.termsets import termset\n",
    "# ts = termset(\"en_clinical\")\n",
    "# print(ts.get_patterns())\n",
    "# ts.add_patterns({\n",
    "#             \"pseudo_negations\": [],\n",
    "#             \"termination\": [],\n",
    "#             \"preceding_negations\": [\"was no\", \"were no\", \"were never\", \"showed no\", \"a negative\"],\n",
    "#             \"following_negations\": [\"was negative\", \"remained negative\", \"was undetectable\"],\n",
    "#         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # from negspacy.negation import Negex\n",
    "\n",
    "# doc = nlp(df['content_text'][2])\n",
    "# # nlp = spacy.load(\"en_core_web_sm\")\n",
    "# # nlp.add_pipe(\"negex\")\n",
    "\n",
    "# # doc = nlp(\"She does not like Steve Jobs but likes Apple products.\")\n",
    "# for e in doc.ents:\n",
    "#     if e._.negex == True:\n",
    "#         print(doc[e.start:e.end])\n",
    "#         print(e.sent)\n",
    "#         print('\\n')\n",
    "# #     print(e.text, e._.negex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcampbell/miniconda3/envs/gopher/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/home/jcampbell/miniconda3/envs/gopher/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 1427 complete\n",
      "10 of 1427 complete\n",
      "20 of 1427 complete\n",
      "30 of 1427 complete\n",
      "40 of 1427 complete\n",
      "50 of 1427 complete\n",
      "60 of 1427 complete\n",
      "70 of 1427 complete\n",
      "80 of 1427 complete\n",
      "90 of 1427 complete\n",
      "100 of 1427 complete\n",
      "110 of 1427 complete\n",
      "120 of 1427 complete\n",
      "130 of 1427 complete\n",
      "140 of 1427 complete\n",
      "150 of 1427 complete\n",
      "160 of 1427 complete\n",
      "170 of 1427 complete\n",
      "180 of 1427 complete\n",
      "190 of 1427 complete\n",
      "200 of 1427 complete\n",
      "210 of 1427 complete\n",
      "220 of 1427 complete\n",
      "230 of 1427 complete\n",
      "240 of 1427 complete\n",
      "250 of 1427 complete\n",
      "260 of 1427 complete\n",
      "270 of 1427 complete\n",
      "280 of 1427 complete\n",
      "290 of 1427 complete\n",
      "300 of 1427 complete\n",
      "310 of 1427 complete\n",
      "320 of 1427 complete\n",
      "330 of 1427 complete\n",
      "340 of 1427 complete\n",
      "350 of 1427 complete\n",
      "360 of 1427 complete\n",
      "370 of 1427 complete\n",
      "380 of 1427 complete\n",
      "390 of 1427 complete\n",
      "400 of 1427 complete\n",
      "410 of 1427 complete\n",
      "420 of 1427 complete\n",
      "430 of 1427 complete\n",
      "440 of 1427 complete\n",
      "450 of 1427 complete\n",
      "460 of 1427 complete\n",
      "470 of 1427 complete\n",
      "480 of 1427 complete\n",
      "490 of 1427 complete\n",
      "500 of 1427 complete\n",
      "510 of 1427 complete\n",
      "520 of 1427 complete\n",
      "530 of 1427 complete\n",
      "540 of 1427 complete\n",
      "550 of 1427 complete\n",
      "560 of 1427 complete\n",
      "570 of 1427 complete\n",
      "580 of 1427 complete\n",
      "590 of 1427 complete\n",
      "600 of 1427 complete\n",
      "610 of 1427 complete\n",
      "620 of 1427 complete\n",
      "630 of 1427 complete\n",
      "640 of 1427 complete\n",
      "650 of 1427 complete\n",
      "660 of 1427 complete\n",
      "670 of 1427 complete\n",
      "680 of 1427 complete\n",
      "690 of 1427 complete\n",
      "700 of 1427 complete\n",
      "710 of 1427 complete\n",
      "720 of 1427 complete\n",
      "730 of 1427 complete\n",
      "740 of 1427 complete\n",
      "750 of 1427 complete\n",
      "760 of 1427 complete\n",
      "770 of 1427 complete\n",
      "780 of 1427 complete\n",
      "790 of 1427 complete\n",
      "800 of 1427 complete\n",
      "810 of 1427 complete\n",
      "820 of 1427 complete\n",
      "830 of 1427 complete\n",
      "840 of 1427 complete\n",
      "850 of 1427 complete\n",
      "860 of 1427 complete\n",
      "870 of 1427 complete\n",
      "880 of 1427 complete\n",
      "890 of 1427 complete\n",
      "900 of 1427 complete\n",
      "910 of 1427 complete\n",
      "920 of 1427 complete\n",
      "930 of 1427 complete\n",
      "940 of 1427 complete\n",
      "950 of 1427 complete\n",
      "960 of 1427 complete\n",
      "970 of 1427 complete\n",
      "980 of 1427 complete\n",
      "990 of 1427 complete\n",
      "1000 of 1427 complete\n",
      "1010 of 1427 complete\n",
      "1020 of 1427 complete\n",
      "1030 of 1427 complete\n",
      "1040 of 1427 complete\n",
      "1050 of 1427 complete\n",
      "1060 of 1427 complete\n",
      "1070 of 1427 complete\n",
      "1080 of 1427 complete\n",
      "1090 of 1427 complete\n",
      "1100 of 1427 complete\n",
      "1110 of 1427 complete\n",
      "1120 of 1427 complete\n",
      "1130 of 1427 complete\n",
      "1140 of 1427 complete\n",
      "1150 of 1427 complete\n",
      "1160 of 1427 complete\n",
      "1170 of 1427 complete\n",
      "1180 of 1427 complete\n",
      "1190 of 1427 complete\n",
      "1200 of 1427 complete\n",
      "1210 of 1427 complete\n",
      "1220 of 1427 complete\n",
      "1230 of 1427 complete\n",
      "1240 of 1427 complete\n",
      "1250 of 1427 complete\n",
      "1260 of 1427 complete\n",
      "1270 of 1427 complete\n",
      "1280 of 1427 complete\n",
      "1290 of 1427 complete\n",
      "1300 of 1427 complete\n",
      "1310 of 1427 complete\n",
      "1320 of 1427 complete\n",
      "1330 of 1427 complete\n",
      "1340 of 1427 complete\n",
      "1350 of 1427 complete\n",
      "1360 of 1427 complete\n",
      "1370 of 1427 complete\n",
      "1380 of 1427 complete\n",
      "1390 of 1427 complete\n",
      "1400 of 1427 complete\n",
      "1410 of 1427 complete\n",
      "1420 of 1427 complete\n"
     ]
    }
   ],
   "source": [
    "hpo_ents = {}\n",
    "for counter, doc in enumerate(nlp.pipe(list(df['content_text']), batch_size =100)):\n",
    "    # every doc will get a list entities that map to HPO\n",
    "    # each entity will have a dictionary with a series of values\n",
    "    hpos = []\n",
    "    # iterate through each entity in the doc\n",
    "    for entity in doc.ents:\n",
    "        # try mapping to HPO\n",
    "        linker = nlp.get_pipe(\"scispacy_linker\")\n",
    "        if entity._.kb_ents:\n",
    "            # if there is a \"match/mention\" you get a cui\n",
    "            mention = entity._.kb_ents[0]\n",
    "            if mention:\n",
    "                # then we get the cui details (mainly the name)\n",
    "                hpo = linker.kb.cui_to_entity[mention[0]]\n",
    "                if hpo is not None:\n",
    "                    # the result is a sring that needs a bit of parsing\n",
    "                    hpo = str(hpo).split('\\n')[0].split(', ')\n",
    "                    # we then store attributes for each entity starting with HPO CUI\n",
    "                    hpos.append({'cui':hpo[0].replace('CUI: ',''),\n",
    "                                 # then we add the primary long form name\n",
    "                                 'name':hpo[1].replace('Name: ',''),\n",
    "                                 # then we add the start and end point of the entity (span)\n",
    "                                 'span':(entity.start, entity.end),\n",
    "                                 # then the score given for the confidence of the match\n",
    "                                 'hpo_thresh':mention[1],\n",
    "                                 # negation of the entity bool\n",
    "                                 'negation':entity._.negex,\n",
    "                                 # the sentiment for the sentence the entity is from\n",
    "                                 'polarity': entity.sent._.polarity,\n",
    "                                 # if the sentence is mostly opinion or fact\n",
    "                                 'subjectivity':entity.sent._.subjectivity\n",
    "                                })\n",
    "    # now we add the ent_d list to our df_d               \n",
    "    hpo_ents.update({df.index[counter]:hpos})\n",
    "    if counter % 10 == 0:\n",
    "        print(f'{counter} of {len(df)} complete')\n",
    "# for each hpo we want the cui, name and threshold\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_to_df(ent_list):\n",
    "    cols = list(ent_list[0].keys())\n",
    "    # holding lists\n",
    "    cuis = []\n",
    "    names = []\n",
    "    spans =[]\n",
    "    hpo_thresh = []\n",
    "    negation = []\n",
    "    polarity = []\n",
    "    subjectivity = []\n",
    "    for val in ent_list:\n",
    "        cuis.append(val['cui'])\n",
    "        names.append(val['name'])\n",
    "        spans.append(val['span'])\n",
    "        hpo_thresh.append(val['hpo_thresh'])\n",
    "        negation.append(val['negation'])\n",
    "        polarity.append(val['polarity'])\n",
    "        subjectivity.append(val['subjectivity'])\n",
    "    df = pd.DataFrame(data = {'cui':cuis,\n",
    "                              'name':names,\n",
    "                              'span':spans,\n",
    "                              'hpo_thresh':hpo_thresh,\n",
    "                              'negation':negation,\n",
    "                              'polarity':polarity,\n",
    "                              'subjectivity':subjectivity})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_df = val_to_df(hpo_ents[df.index[1]])\n",
    "len(ent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Increased number of platelets in blood', 10),\n",
       " ('Abnormality of the cerebrospinal fluid', 10),\n",
       " ('Lymphocytopenia', 8),\n",
       " ('Severe', 7),\n",
       " ('Fever', 7),\n",
       " ('Low platelet count', 6),\n",
       " ('Low neutrophil count', 5),\n",
       " ('Irritability', 4),\n",
       " ('Bone infection', 4),\n",
       " ('Thrombocytopenia', 4)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ent_df['name']).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetic ketoacidosis at the onset of T1DM was more frequent, and a higher percentage of these patients presented with a severe form.\n",
      "\n",
      "\n",
      "Several studies reported a significant increase in the frequency of DKA present at the diagnosis of T1DM, with a higher percentage of the severe form of this metabolic complication [,,,,,].\n",
      "\n",
      "\n",
      "The degree of severity of DKA was classified according to 2018 International Society for Paediatric and Adolescent Diabetes (ISPAD) guidelines: mild DKA (pH < ), moderate DKA (pH < ), severe DKA (pH < ).\n",
      "\n",
      "\n",
      "In the pandemic group, a higher percent of DKA cases developed the severe form compared to the pre-pandemic group ( vs. , OR = , CI 95% = â€“, p = ). (, ).\n",
      "\n",
      "\n",
      "Among the eight patients, seven presented DKA which was mild in three cases, moderate in three, and severe in one patient.\n",
      "\n",
      "\n",
      "Moreover, a higher percentage of children presented a severe form of this metabolic complication.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in hpos:\n",
    "    if d['name'] == 'Severe':\n",
    "        print(doc[d['span'][0]].sent)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(hpo_ents, open('./umls_ents.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-79-bf186ad93750>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-79-bf186ad93750>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    create ent_sents\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "create ent_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gopher]",
   "language": "python",
   "name": "conda-env-gopher-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
