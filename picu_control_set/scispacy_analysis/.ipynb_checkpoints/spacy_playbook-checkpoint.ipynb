{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcampbell/miniconda3/envs/gopher/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/home/jcampbell/miniconda3/envs/gopher/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import spacy\n",
    "import scispacy\n",
    "from collections import Counter\n",
    "import pathlib\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from scispacy.linking import EntityLinker\n",
    "from negspacy.negation import Negex\n",
    "from negspacy.termsets import termset\n",
    "ts = termset(\"en_clinical\")\n",
    "\n",
    "# Intantiate the spacy program specifying the model you want to use\n",
    "nlp = spacy.load(\"en_core_sci_lg\")\n",
    "\n",
    "# Add the abbreviation pipe to the spacy pipeline.\n",
    "nlp.add_pipe(\"abbreviation_detector\")\n",
    "# Add the hpo entity linker pipe to the spacy pipeline.\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True,\n",
    "                                        \"linker_name\": \"umls\",\n",
    "                                        'threshold':0.85,\n",
    "                                        'max_entities_per_mention':1\n",
    "                                       })\n",
    "# add the sentiment analysis step\n",
    "nlp.add_pipe('spacytextblob')\n",
    "# add the negation detection\n",
    "nlp.add_pipe(\"negex\")\n",
    "ts.add_patterns({\n",
    "            \"pseudo_negations\": [],\n",
    "            \"termination\": [],\n",
    "            \"preceding_negations\": [\"was no\", \"were no\", \"were never\", \"showed no\", \"a negative\"],\n",
    "            \"following_negations\": [\"was negative\", \"remained negative\", \"was undetectable\"],\n",
    "        })\n",
    "\n",
    "# nlp.remove_pipe(\"scispacy_linker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1427\n"
     ]
    }
   ],
   "source": [
    "paed_path = '/home/jcampbell/paed_covid_case_reports/picu_control_set/output/retrieved_df/retrieved_df2.p'\n",
    "df = pickle.load(open(paed_path, 'rb'))   \n",
    "condition = [type(text) == str for text in df['content_text']]\n",
    "df=pd.DataFrame(df.loc[condition,:])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # abbreviations\n",
    "# abrvs = {'long_forms':[],'spans':[]}\n",
    "# for abrv in doc._.abbreviations:\n",
    "#     abrvs['long_forms'].append(abrv._.long_form)\n",
    "#     abrvs['spans'].append((abrv.start,abrv.end))\n",
    "    \n",
    "# Counter(abrvs['long_forms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # text = 'I had a really horrible day. It was the worst day ever! But every now and then I have a really good day that makes me happy.'\n",
    "# doc = nlp(df['content_text'][1])\n",
    "# for sent in doc.sents:\n",
    "#     polarity = sent._.polarity\n",
    "#     if polarity < -0.5:\n",
    "#         print(sent)\n",
    "#         print(sent._.assessments)\n",
    "# # ent = doc.ents[10]\n",
    "# # print(ent)\n",
    "# # # doc._.polarity      # Polarity: -0.125\n",
    "# # # doc._.subjectivity  # Sujectivity: 0.9\n",
    "# # # doc._.assessments   # Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\n",
    "# # ent._.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from negspacy.termsets import termset\n",
    "# ts = termset(\"en_clinical\")\n",
    "# print(ts.get_patterns())\n",
    "# ts.add_patterns({\n",
    "#             \"pseudo_negations\": [],\n",
    "#             \"termination\": [],\n",
    "#             \"preceding_negations\": [\"was no\", \"were no\", \"were never\", \"showed no\", \"a negative\"],\n",
    "#             \"following_negations\": [\"was negative\", \"remained negative\", \"was undetectable\"],\n",
    "#         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # from negspacy.negation import Negex\n",
    "\n",
    "# doc = nlp(df['content_text'][2])\n",
    "# # nlp = spacy.load(\"en_core_web_sm\")\n",
    "# # nlp.add_pipe(\"negex\")\n",
    "\n",
    "# # doc = nlp(\"She does not like Steve Jobs but likes Apple products.\")\n",
    "# for e in doc.ents:\n",
    "#     if e._.negex == True:\n",
    "#         print(doc[e.start:e.end])\n",
    "#         print(e.sent)\n",
    "#         print('\\n')\n",
    "# #     print(e.text, e._.negex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 1427 complete\n",
      "10 of 1427 complete\n",
      "20 of 1427 complete\n",
      "30 of 1427 complete\n",
      "40 of 1427 complete\n",
      "50 of 1427 complete\n",
      "60 of 1427 complete\n",
      "70 of 1427 complete\n",
      "80 of 1427 complete\n",
      "90 of 1427 complete\n",
      "100 of 1427 complete\n",
      "110 of 1427 complete\n",
      "120 of 1427 complete\n",
      "130 of 1427 complete\n",
      "140 of 1427 complete\n",
      "150 of 1427 complete\n",
      "160 of 1427 complete\n",
      "170 of 1427 complete\n",
      "180 of 1427 complete\n",
      "190 of 1427 complete\n",
      "200 of 1427 complete\n",
      "210 of 1427 complete\n",
      "220 of 1427 complete\n",
      "230 of 1427 complete\n",
      "240 of 1427 complete\n",
      "250 of 1427 complete\n",
      "260 of 1427 complete\n"
     ]
    }
   ],
   "source": [
    "hpo_ents = {}\n",
    "for counter, doc in enumerate(nlp.pipe(list(df['content_text']), batch_size =100)):\n",
    "    # every doc will get a list entities that map to HPO\n",
    "    # each entity will have a dictionary with a series of values\n",
    "    hpos = []\n",
    "    # iterate through each entity in the doc\n",
    "    for entity in doc.ents:\n",
    "        # try mapping to HPO\n",
    "        linker = nlp.get_pipe(\"scispacy_linker\")\n",
    "        if entity._.kb_ents:\n",
    "            # if there is a \"match/mention\" you get a cui\n",
    "            mention = entity._.kb_ents[0]\n",
    "            if mention:\n",
    "                # then we get the cui details (mainly the name)\n",
    "                hpo = linker.kb.cui_to_entity[mention[0]]\n",
    "                if hpo is not None:\n",
    "                    # the result is a sring that needs a bit of parsing\n",
    "                    hpo = str(hpo).split('\\n')[0].split(', ')\n",
    "                    # we then store attributes for each entity starting with HPO CUI\n",
    "                    hpos.append({'cui':hpo[0].replace('CUI: ',''),\n",
    "                                 # then we add the primary long form name\n",
    "                                 'name':hpo[1].replace('Name: ',''),\n",
    "                                 # then we add the start and end point of the entity (span)\n",
    "                                 'span':(entity.start, entity.end),\n",
    "                                 # then the score given for the confidence of the match\n",
    "                                 'hpo_thresh':mention[1],\n",
    "                                 # negation of the entity bool\n",
    "                                 'negation':entity._.negex,\n",
    "                                 # the sentiment for the sentence the entity is from\n",
    "                                 'polarity': entity.sent._.polarity,\n",
    "                                 # if the sentence is mostly opinion or fact\n",
    "                                 'subjectivity':entity.sent._.subjectivity\n",
    "                                })\n",
    "    # now we add the ent_d list to our df_d               \n",
    "    hpo_ents.update({df.index[counter]:hpos})\n",
    "    if counter % 10 == 0:\n",
    "        print(f'{counter} of {len(df)} complete')\n",
    "# for each hpo we want the cui, name and threshold\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_to_df(ent_list):\n",
    "    cols = list(ent_list[0].keys())\n",
    "    # holding lists\n",
    "    cuis = []\n",
    "    names = []\n",
    "    spans =[]\n",
    "    hpo_thresh = []\n",
    "    negation = []\n",
    "    polarity = []\n",
    "    subjectivity = []\n",
    "    for val in ent_list:\n",
    "        cuis.append(val['cui'])\n",
    "        names.append(val['name'])\n",
    "        spans.append(val['span'])\n",
    "        hpo_thresh.append(val['hpo_thresh'])\n",
    "        negation.append(val['negation'])\n",
    "        polarity.append(val['polarity'])\n",
    "        subjectivity.append(val['subjectivity'])\n",
    "    df = pd.DataFrame(data = {'cui':cuis,\n",
    "                              'name':names,\n",
    "                              'span':spans,\n",
    "                              'hpo_thresh':hpo_thresh,\n",
    "                              'negation':negation,\n",
    "                              'polarity':polarity,\n",
    "                              'subjectivity':subjectivity})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_df = val_to_df(hpo_ents[df.index[1]])\n",
    "len(ent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Increased number of platelets in blood', 10),\n",
       " ('Abnormality of the cerebrospinal fluid', 10),\n",
       " ('Lymphocytopenia', 8),\n",
       " ('Severe', 7),\n",
       " ('Fever', 7),\n",
       " ('Low platelet count', 6),\n",
       " ('Low neutrophil count', 5),\n",
       " ('Irritability', 4),\n",
       " ('Bone infection', 4),\n",
       " ('Thrombocytopenia', 4)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ent_df['name']).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetic ketoacidosis at the onset of T1DM was more frequent, and a higher percentage of these patients presented with a severe form.\n",
      "\n",
      "\n",
      "Several studies reported a significant increase in the frequency of DKA present at the diagnosis of T1DM, with a higher percentage of the severe form of this metabolic complication [,,,,,].\n",
      "\n",
      "\n",
      "The degree of severity of DKA was classified according to 2018 International Society for Paediatric and Adolescent Diabetes (ISPAD) guidelines: mild DKA (pH < ), moderate DKA (pH < ), severe DKA (pH < ).\n",
      "\n",
      "\n",
      "In the pandemic group, a higher percent of DKA cases developed the severe form compared to the pre-pandemic group ( vs. , OR = , CI 95% = â€“, p = ). (, ).\n",
      "\n",
      "\n",
      "Among the eight patients, seven presented DKA which was mild in three cases, moderate in three, and severe in one patient.\n",
      "\n",
      "\n",
      "Moreover, a higher percentage of children presented a severe form of this metabolic complication.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in hpos:\n",
    "    if d['name'] == 'Severe':\n",
    "        print(doc[d['span'][0]].sent)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(hpo_ents, open('./umls_ents.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-79-bf186ad93750>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-79-bf186ad93750>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    create ent_sents\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "create ent_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gopher]",
   "language": "python",
   "name": "conda-env-gopher-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
