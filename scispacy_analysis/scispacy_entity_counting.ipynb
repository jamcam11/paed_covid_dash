{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Notebook Aims to use sci-spacy models to extract scientific entities from a corpus of text. \n",
    "# I am using the large vocabulary model \n",
    "# i see there is ongoing development including a scibert model that we should probably look at\n",
    "# our aim is to summarise a the corpus generated by a search query in pubmed and compare\n",
    "# Hopefully the comparison will show that we have retrieved and extracted useful texts \n",
    "\n",
    "### Tasks ####\n",
    "# find the required Dataframe of working texts for a given corpus\n",
    "# Install and load the spacy models for use.\n",
    "# clean the text ready for token interpretation (this might not been needed with spacy)\n",
    "# get the spacy ents for all docs -\n",
    "# i would like to create a count vector for as part of this process to allow better summarisation. \n",
    "# look at TF-IDF scores etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate a conda env\n",
    "# pip install scispacy\n",
    "# pick your model from here https://allenai.github.io/scispacy/\n",
    "# pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_lg-0.4.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import spacy\n",
    "import scispacy\n",
    "from collections import Counter\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intantiate the spacy program specifying the model you want to use\n",
    "nlp = spacy.load(\"en_core_sci_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_data2 = pickle.load(open('./docs.p', 'rb'))\n",
    "# docs2 = pickle.loads(doc_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to install a trained pipeline, e.g.: python -m spacy download e\n",
    "\n",
    "# displacy.render(docs2[0][:100], style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the directory that holds our df of intereset (master_df2)\n",
    "\n",
    "paed_path = '/home/jcampbell/paed_covid_case_reports/output/retrieved_df/retrieved_df2.p'\n",
    "df = pickle.load(open(paed_path, 'rb'))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cui_df = pickle.load(open('./cui_df.p', 'rb'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indexes</th>\n",
       "      <th>hpo_str</th>\n",
       "      <th>sents</th>\n",
       "      <th>negation</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C0205082</th>\n",
       "      <td>[e1d20cf546fb43f9a82016dabdd95518, e1d20cf546f...</td>\n",
       "      <td>Severe</td>\n",
       "      <td>[To the Editor: Severe acute respiratory syndr...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[0.265, 0.2375, 0.16666666666666666, 0.1666666...</td>\n",
       "      <td>[0.5016666666666667, 0.6375, 0.583333333333333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C0006826</th>\n",
       "      <td>[e1d20cf546fb43f9a82016dabdd95518]</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>[Older age and co-morbidities such as cancer, ...</td>\n",
       "      <td>[False]</td>\n",
       "      <td>[0.16666666666666666]</td>\n",
       "      <td>[0.5833333333333333]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C0007222</th>\n",
       "      <td>[e1d20cf546fb43f9a82016dabdd95518]</td>\n",
       "      <td>Cardiovascular disease</td>\n",
       "      <td>[Older age and co-morbidities such as cancer, ...</td>\n",
       "      <td>[False]</td>\n",
       "      <td>[0.16666666666666666]</td>\n",
       "      <td>[0.5833333333333333]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C0449259</th>\n",
       "      <td>[e1d20cf546fb43f9a82016dabdd95518, 91ed9b09ceb...</td>\n",
       "      <td>Clinical course</td>\n",
       "      <td>[A larger international retrospective cohort r...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[0.05572916666666666, -0.16666666666666666, -0...</td>\n",
       "      <td>[0.3875, 0.06666666666666667, 0.33333333333333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C0085595</th>\n",
       "      <td>[e1d20cf546fb43f9a82016dabdd95518, 91ed9b09ceb...</td>\n",
       "      <td>Body odor</td>\n",
       "      <td>[The patient is a 12-month-old African-America...</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>[-0.0625, 0.1, -0.35, 0.11249999999999999, 0.0]</td>\n",
       "      <td>[0.25, 0.35, 0.4, 0.4083333333333333, 0.066666...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    indexes  \\\n",
       "C0205082  [e1d20cf546fb43f9a82016dabdd95518, e1d20cf546f...   \n",
       "C0006826                 [e1d20cf546fb43f9a82016dabdd95518]   \n",
       "C0007222                 [e1d20cf546fb43f9a82016dabdd95518]   \n",
       "C0449259  [e1d20cf546fb43f9a82016dabdd95518, 91ed9b09ceb...   \n",
       "C0085595  [e1d20cf546fb43f9a82016dabdd95518, 91ed9b09ceb...   \n",
       "\n",
       "                         hpo_str  \\\n",
       "C0205082                  Severe   \n",
       "C0006826                  Cancer   \n",
       "C0007222  Cardiovascular disease   \n",
       "C0449259         Clinical course   \n",
       "C0085595               Body odor   \n",
       "\n",
       "                                                      sents  \\\n",
       "C0205082  [To the Editor: Severe acute respiratory syndr...   \n",
       "C0006826  [Older age and co-morbidities such as cancer, ...   \n",
       "C0007222  [Older age and co-morbidities such as cancer, ...   \n",
       "C0449259  [A larger international retrospective cohort r...   \n",
       "C0085595  [The patient is a 12-month-old African-America...   \n",
       "\n",
       "                                                   negation  \\\n",
       "C0205082  [False, False, False, False, False, False, Fal...   \n",
       "C0006826                                            [False]   \n",
       "C0007222                                            [False]   \n",
       "C0449259  [False, False, False, False, False, False, Fal...   \n",
       "C0085595                [False, False, False, False, False]   \n",
       "\n",
       "                                                   polarity  \\\n",
       "C0205082  [0.265, 0.2375, 0.16666666666666666, 0.1666666...   \n",
       "C0006826                              [0.16666666666666666]   \n",
       "C0007222                              [0.16666666666666666]   \n",
       "C0449259  [0.05572916666666666, -0.16666666666666666, -0...   \n",
       "C0085595    [-0.0625, 0.1, -0.35, 0.11249999999999999, 0.0]   \n",
       "\n",
       "                                                       subj  \n",
       "C0205082  [0.5016666666666667, 0.6375, 0.583333333333333...  \n",
       "C0006826                               [0.5833333333333333]  \n",
       "C0007222                               [0.5833333333333333]  \n",
       "C0449259  [0.3875, 0.06666666666666667, 0.33333333333333...  \n",
       "C0085595  [0.25, 0.35, 0.4, 0.4083333333333333, 0.066666...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cui_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = [len(set(index)) for index in cui_df['indexes']]\n",
    "max(lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severe\n",
      "18\n",
      "7\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# I will subset the master_df to build the process for NLP analysis.\n",
    "\n",
    "# objectives\n",
    "# create and array of spcay docs that links to the index of each row in the master df \n",
    "# ? a dictionary best for this\n",
    "\n",
    "# each entity should be linked to a sentence for context display (? sentence index for memory saving)\n",
    "# need to see if we can get entity count rather than just presence or absence \n",
    "#     - maybe doing analysis per sentence will allow this\n",
    "# for each doc we want a dictionary of entities \n",
    "# {entity: :1, sent_i:0, }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [len(doc) for doc in df['content_text'] if doc is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 docs had over 1M characters\n",
      "0 docs had were absent\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import spacy\n",
    "import scispacy\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "i = 0\n",
    "too_big = 0\n",
    "empty = 0\n",
    "nlp_docs = {}\n",
    "for index, row in df1.iterrows():\n",
    "    i+=1\n",
    "    if len(row['content_text'].split()) > 50000:\n",
    "        doc = 'Too Big'\n",
    "        too_big+=1\n",
    "\n",
    "    elif type(row['content_text']) != str or row['content_text'] == '':\n",
    "        doc = None\n",
    "        empty +=1\n",
    "    else:\n",
    "        doc = nlp(row['content_text'])\n",
    "    nlp_docs.update({index: doc})\n",
    "\n",
    "pickle.dump(nlp_docs, open(f'./paeds_docs_dict.p', 'wb'))\n",
    "\n",
    "print(f'{too_big} docs had over 50k words')\n",
    "print(f'{empty} docs had were absent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_d = pickle.load(open('paeds_docs_dict.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Accesses, 1),\n",
       " (Citations, 1),\n",
       " (systematic review, 1),\n",
       " (clinical features, 1),\n",
       " (maternal, 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents = []\n",
    "for doc in doc_d.values():\n",
    "    for ent in doc.ents:\n",
    "        ents.append(ent)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accesses\n",
      "Accesses\n",
      "ENTITY\n",
      "1\n",
      "2\n",
      "NNS\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "print(ents[0])\n",
    "print(ents[0].text)\n",
    "print(ents[0].label_)\n",
    "print(ents[0].start)\n",
    "print(ents[0].end)\n",
    "doc = doc_d[keys[0]]\n",
    "\n",
    "print(doc[1].tag_)\n",
    "print(doc[1].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i've saved the spacy docs in a dictionary, one entry per row in the master df. \n",
    "# This should allow us to loop through and pull out entities and tie them to sentences at the doc level.\n",
    "# now just need to build a more useful structure from the docs.\n",
    "# one that allows aggregation of entities etc across the whole corpus.\n",
    "# ideally we should be able to create a count vector that would work across these structures\n",
    "\n",
    "# lets read in one of these spacy doc dictionaries to build the structure.\n",
    "paed_docs = pickle.load(open('./paed_docs_dict.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF vectors for each sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1925 spaCy docs to evaluate from the fgfr3 corpus\n",
      "there are 331257 unique terms in the fgfr3 corpus.\n",
      "We have 1972 spaCy docs to evaluate from the mecp2 corpus\n",
      "there are 357889 unique terms in the mecp2 corpus.\n",
      "We have 2572 spaCy docs to evaluate from the psen1 corpus\n",
      "there are 384535 unique terms in the psen1 corpus.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for topic in ['paed']:\n",
    "    \n",
    "    docs = pickle.load(open(f'./{topic}_docs_dict.p', 'rb'))\n",
    "    \n",
    "    print(f'We have {len(docs.keys())} spaCy docs to evaluate from the {topic} corpus')\n",
    "    # average number of Sci-spaCy entities extracted from each doc in the corpus\n",
    "    # list entities per doc with the sentences they came from\n",
    "\n",
    "    # we'll store the output into a dictionary that will be able to make a df once complete. \n",
    "    # using the index as the main id.\n",
    "    eval_d = {}\n",
    "\n",
    "    # we'll also generate an entity orientated dictionary so that we can query the corpus based on entities\n",
    "    ent_d = {}\n",
    "\n",
    "    # loop through the dictionary of indexes and spaCy docs\n",
    "    doc_count = 0\n",
    "    for index, doc in docs.items():\n",
    "        doc_count +=1\n",
    "        # make sure the doc is not empty/None\n",
    "        if doc is not None:\n",
    "            # we'll save a list of split sentences for context printing later\n",
    "            sentences = []\n",
    "            # we'll also just store every entity as a flat list for each doc\n",
    "            flat_doc_ents = []\n",
    "            # we'll need an increment counter for each sentence index\n",
    "            sent_i = 0\n",
    "            for sent in doc.sents: \n",
    "                sentences.append(str(sent))\n",
    "\n",
    "                # get all the entities in that sentence (each one will have the same sent_i)\n",
    "                for ent in sent.ents:\n",
    "                    # for each entity check if it is already in the entities dict for the doc\n",
    "                    # if it is not in the dict then add a new {key:value}\n",
    "                    # if it is in the dict then add the sentence index to the entity index list\n",
    "                    if str(ent) not in ent_d.keys():\n",
    "                        ent_d.update({str(ent):{'locs':[(index,sent_i)]}})\n",
    "                    else:\n",
    "                        ent_d[str(ent)]['locs'].extend((index, sent_i))\n",
    "                    flat_doc_ents.append(str(ent))\n",
    "                # increment the count index\n",
    "                sent_i +=1\n",
    "\n",
    "            # once all the sentences have been evaluated . . .\n",
    "            eval_d.update({index:{'sents':sentences,\n",
    "                                  'flat_ents':flat_doc_ents}})\n",
    "\n",
    "        # when the working text is empty, spaCy doc = None\n",
    "        else:\n",
    "            # now add the sentences and entities to the eval d \n",
    "            eval_d.update({index:{'sents':None,\n",
    "                                  'flat_ents':None}})\n",
    "\n",
    "#         percent = np.round(doc_count/len(docs.keys()),2)*100\n",
    "#         if percent%10 == 0:\n",
    "#             print(f'working on doc {doc_count} of {len(fmr1_docs.keys())}')\n",
    "\n",
    "    # now for each eval_d index we want to generate an entity count so we can sort by most common. \n",
    "\n",
    "    # at the same time i'll create one flat file of entities for the whole corpus\n",
    "    # so we can get total counts for each entity and a vocabular of all entities\n",
    "    corpus_flat_ents = []\n",
    "    for index in eval_d.keys():\n",
    "        flat_ents = eval_d[index]['flat_ents']\n",
    "        if flat_ents is not None:\n",
    "            corpus_flat_ents.extend([str(ent) for ent in flat_ents])\n",
    "            count_d = Counter(flat_ents)\n",
    "            eval_d[index].update({'counter':count_d})\n",
    "\n",
    "\n",
    "    vocab = list(set(corpus_flat_ents))\n",
    "    print(f'there are {len(vocab)} unique terms in the {gene} corpus.')\n",
    "    \n",
    "    # now lets consider the total count for each entity\n",
    "    # now use counter to look for each unique entity\n",
    "    count_d = dict(Counter(corpus_flat_ents))\n",
    "\n",
    "    corpus_ents = []\n",
    "    # document frequency counter for each entity\n",
    "    # use a counter after we create a flat list(set(ent list))\n",
    "    for index in eval_d.keys():\n",
    "        flat_ents = eval_d[index]['flat_ents']\n",
    "        if flat_ents is not None:\n",
    "            corpus_ents.extend(list(set(flat_ents)))\n",
    "\n",
    "    binary_counts = dict(Counter(corpus_ents))\n",
    "\n",
    "    # map each entity on to the ent_d to add the count_freq\n",
    "    for ent in vocab:\n",
    "        ent_d[ent].update({'count_freq':count_d[ent],\n",
    "                          'doc_freq':binary_counts[ent]})\n",
    "    pickle.dump(ent_d, open(f'./{gene}_ent_d.p', 'wb'))\n",
    "    pickle.dump(eval_d, open(f'./{gene}_eval_d.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gopher]",
   "language": "python",
   "name": "conda-env-gopher-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
