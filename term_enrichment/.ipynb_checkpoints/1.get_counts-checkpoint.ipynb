{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This Notebook aims to generate frequency counts for each term based on training data.\n",
    "Each term will have a count for presence or absence in a document for each class. \n",
    "We need to set the Corpus we are working from and at the end, write a dataframe to file for use in anther notebook. The next step will be to look at the enrichment of each term by class. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Import libraries and write settings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T12:31:31.539923Z",
     "start_time": "2020-02-26T12:31:28.887599Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import scipy.stats as stats\n",
    "from Bio import Entrez\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import snowballstemmer\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "import math\n",
    "\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report as report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4217"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets import the chosen corpus into a dataframe \n",
    "df = pickle.load(open(f'../master_df_2020_07_16.p', 'rb'))\n",
    "# df['reclass'].fillna(0, inplace = True, axis = 0)\n",
    "df.dropna(subset=['reclass'], axis = 0, inplace = True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the training set = 2484\n",
      "Length of the test set = 500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df.dropna(subset=['reclass'], axis = 0, inplace = True)\n",
    "# print(f'There are {len(df)} records in the manually classified corpus')# i want to split the corpus into a train and test set \n",
    "# ********* NB would be good to build in K fold cross validation to this step *****************\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=500, random_state = 23)\n",
    "\n",
    "pickle.dump(train_df, open(f'train_df.p', 'wb'))\n",
    "pickle.dump(test_df, open(f'test_df.p', 'wb'))\n",
    "\n",
    "print(f'Length of the training set = {len(train_df)}')\n",
    "print(f'Length of the test set = {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3717"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(test_df.index, inplace = True, axis = 0)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T12:31:47.531493Z",
     "start_time": "2020-02-26T12:31:46.721052Z"
    }
   },
   "outputs": [],
   "source": [
    "# # lets import the chosen corpus into a dataframe \n",
    "# df = pickle.load(open(f'../class_df.p', 'rb'))\n",
    "# df['reclass'].fillna(0, inplace = True, axis = 0)\n",
    "# print(f'There are {len(df)} records in the manually classified corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T14:52:58.220831Z",
     "start_time": "2020-02-25T14:52:57.284281Z"
    }
   },
   "outputs": [],
   "source": [
    "# # i want to split the corpus into a train and test set \n",
    "# # ********* NB would be good to build in K fold cross validation to this step *****************\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=23)\n",
    "\n",
    "# pickle.dump(train_df, open(f'train_df.p', 'wb'))\n",
    "# pickle.dump(test_df, open(f'test_df.p', 'wb'))\n",
    "\n",
    "# print(f'Length of the training set = {len(train_df)}')\n",
    "# print(f'Length of the test set = {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T14:55:09.791154Z",
     "start_time": "2020-02-25T14:55:09.745791Z"
    }
   },
   "outputs": [],
   "source": [
    "# set which column we are looking at\n",
    "# could do this for 'mh_simple_split' or 'clean_TIAB'\n",
    "col = 'working_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T14:55:12.213632Z",
     "start_time": "2020-02-25T14:55:11.384414Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_df = pickle.load(open('./train_df.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T14:55:13.680304Z",
     "start_time": "2020-02-25T14:55:13.619157Z"
    }
   },
   "outputs": [],
   "source": [
    " # import the nltk stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "new_stops =['http',\n",
    "           'doi',\n",
    "           'org',\n",
    "           'medrxiv',\n",
    "           'manuscript',\n",
    "           'preprint',\n",
    "           'license',\n",
    "           'creativecommons',\n",
    "           'et',\n",
    "           'al',\n",
    "           'https',\n",
    "           'ti',\n",
    "           'kw',\n",
    "           'ab',\n",
    "           'nc',\n",
    "           'nd',\n",
    "           'cc',\n",
    "           'yes',\n",
    "           'no',\n",
    "           'www']\n",
    "stop_words = stop_words + new_stops\n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# define our preprocessing to use in the Count vectoriser\n",
    "def preprocessing(text):\n",
    "    \n",
    "    \"\"\"Basic cleaning of texts.\"\"\"\n",
    "    \n",
    "    # remove html markup\n",
    "    text=re.sub(\"(<.*?>)\",\"\",text)\n",
    "    \n",
    "    #remove non-ascii and digits\n",
    "    text=re.sub(\"(\\\\W|\\\\d)\",\" \",text)\n",
    "    \n",
    "    #remove whitespace\n",
    "    text=text.strip().lower()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    \n",
    "    \"\"\"Tokenizing and stemming words\"\"\"\n",
    "    \n",
    "    # split on whitespace\n",
    "    tokens = re.split(\"\\\\s+\",text)\n",
    "    #     # porter stemmer on each token\n",
    "    #     stemmed_words=[porter_stemmer.stem(word = token) for token in tokens if (len(token) > 1) and (token not in stopwords)]\n",
    "    # lemmatize each token\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens if (len(token) > 1) and (token not in stop_words)]\n",
    "    \n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T14:55:14.974182Z",
     "start_time": "2020-02-25T14:55:14.924431Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets set up the Count Vectoriser\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(preprocessor=preprocessing,\n",
    "                     tokenizer=tokenizer,\n",
    "                     lowercase=True,\n",
    "                     binary=True,\n",
    "                     ngram_range=(1,2),\n",
    "                     min_df=1,\n",
    "                     max_df=0.8,\n",
    "                     max_features=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T14:55:16.271014Z",
     "start_time": "2020-02-25T14:55:16.227044Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vec = cv.fit_transform(train_df.working_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cv'] = [vec for vec in count_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T14:55:17.544624Z",
     "start_time": "2020-02-25T14:55:17.498075Z"
    }
   },
   "outputs": [],
   "source": [
    "# we want a to count the number of included documents with each term present\n",
    "# then we'll want to count the number of excluded documents with the term present\n",
    "# this is probably easiest by splitting the training df into an included and excluded df\n",
    "# then we can apply a binary Count vecotriser to each document in each dataframe.\n",
    "\n",
    "# we split the training set to do this quickly and simply, grouped by inclusion and exclusion column 'Human pheno'\n",
    "inc_df = train_df[train_df['reclass'] == 2]\n",
    "ex_df = train_df.drop(inc_df.index, axis = 0, inplace = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T14:55:29.546178Z",
     "start_time": "2020-02-25T14:55:18.779270Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a list of TIAB or MeSH terms for the incuded documents \n",
    "# inc_corpus = inc_df[col]\n",
    "# ex_corpus = ex_df[col]\n",
    "# for each corpus of documents, perform binary counts for every term in the training corpus vocabulary\n",
    "# each document will have a vector of len(vocab) with each terms represented with 0 or 1 for absent or present \n",
    "# inc_count_vec = cv.fit_transform(inc_corpus).toarray()\n",
    "# ex_count_vec = cv.fit_transform(ex_corpus).toarray()\n",
    "# for every term in the vector we can sum the counts for each class \n",
    "# this list comprehension takes every position in the list of vectors \n",
    "# (each list corresponds to a class and each document has its own vector)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inc_term_counts = [sum(t) for t in zip(*inc_df['cv'])]\n",
    "inc_absent_counts = [(len(inc_df) - count) for count in inc_term_counts]\n",
    "ex_term_counts = [sum(t) for t in zip(*ex_df['cv'])]\n",
    "ex_absent_counts = [(len(ex_df) - count) for count in ex_term_counts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T14:55:31.478945Z",
     "start_time": "2020-02-25T14:55:31.404335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pres_inc</th>\n",
       "      <th>abs_inc</th>\n",
       "      <th>pres_ex</th>\n",
       "      <th>abs_ex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>13</td>\n",
       "      <td>306</td>\n",
       "      <td>183</td>\n",
       "      <td>3215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa wilder</th>\n",
       "      <td>2</td>\n",
       "      <td>317</td>\n",
       "      <td>11</td>\n",
       "      <td>3387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaa</th>\n",
       "      <td>0</td>\n",
       "      <td>319</td>\n",
       "      <td>13</td>\n",
       "      <td>3385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aac</th>\n",
       "      <td>1</td>\n",
       "      <td>318</td>\n",
       "      <td>14</td>\n",
       "      <td>3384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aak</th>\n",
       "      <td>0</td>\n",
       "      <td>319</td>\n",
       "      <td>21</td>\n",
       "      <td>3377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pres_inc  abs_inc  pres_ex  abs_ex\n",
       "aa               13      306      183    3215\n",
       "aa wilder         2      317       11    3387\n",
       "aaa               0      319       13    3385\n",
       "aac               1      318       14    3384\n",
       "aak               0      319       21    3377"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can map the term counts onto a dataframe to visualise the count for each class.\n",
    "# can set the index as the term from the count vec dictionary\n",
    "\n",
    "token_df = pd.DataFrame(data={'pres_inc':inc_term_counts,\n",
    "                              'abs_inc':inc_absent_counts,\n",
    "                              'pres_ex':ex_term_counts,\n",
    "                              'abs_ex':ex_absent_counts},\n",
    "                              index=cv.get_feature_names())\n",
    "token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pres_inc</th>\n",
       "      <th>abs_inc</th>\n",
       "      <th>pres_ex</th>\n",
       "      <th>abs_ex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>clinical</th>\n",
       "      <td>284</td>\n",
       "      <td>35</td>\n",
       "      <td>2633</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respiratory</th>\n",
       "      <td>281</td>\n",
       "      <td>38</td>\n",
       "      <td>2519</td>\n",
       "      <td>879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severe</th>\n",
       "      <td>280</td>\n",
       "      <td>39</td>\n",
       "      <td>2465</td>\n",
       "      <td>933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symptom</th>\n",
       "      <td>275</td>\n",
       "      <td>44</td>\n",
       "      <td>2129</td>\n",
       "      <td>1269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>child</th>\n",
       "      <td>274</td>\n",
       "      <td>45</td>\n",
       "      <td>753</td>\n",
       "      <td>2645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reported</th>\n",
       "      <td>266</td>\n",
       "      <td>53</td>\n",
       "      <td>2633</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sars</th>\n",
       "      <td>262</td>\n",
       "      <td>57</td>\n",
       "      <td>2467</td>\n",
       "      <td>931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cov</th>\n",
       "      <td>259</td>\n",
       "      <td>60</td>\n",
       "      <td>2319</td>\n",
       "      <td>1079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study</th>\n",
       "      <td>257</td>\n",
       "      <td>62</td>\n",
       "      <td>2611</td>\n",
       "      <td>787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sars cov</th>\n",
       "      <td>251</td>\n",
       "      <td>68</td>\n",
       "      <td>2180</td>\n",
       "      <td>1218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>251</td>\n",
       "      <td>68</td>\n",
       "      <td>2576</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>250</td>\n",
       "      <td>69</td>\n",
       "      <td>2317</td>\n",
       "      <td>1081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>247</td>\n",
       "      <td>72</td>\n",
       "      <td>1892</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>247</td>\n",
       "      <td>72</td>\n",
       "      <td>2219</td>\n",
       "      <td>1179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may</th>\n",
       "      <td>246</td>\n",
       "      <td>73</td>\n",
       "      <td>2509</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>246</td>\n",
       "      <td>73</td>\n",
       "      <td>2656</td>\n",
       "      <td>742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hospital</th>\n",
       "      <td>244</td>\n",
       "      <td>75</td>\n",
       "      <td>2099</td>\n",
       "      <td>1299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>however</th>\n",
       "      <td>243</td>\n",
       "      <td>76</td>\n",
       "      <td>2333</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>241</td>\n",
       "      <td>78</td>\n",
       "      <td>2660</td>\n",
       "      <td>738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acute</th>\n",
       "      <td>241</td>\n",
       "      <td>78</td>\n",
       "      <td>2261</td>\n",
       "      <td>1137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pres_inc  abs_inc  pres_ex  abs_ex\n",
       "clinical          284       35     2633     765\n",
       "respiratory       281       38     2519     879\n",
       "severe            280       39     2465     933\n",
       "symptom           275       44     2129    1269\n",
       "child             274       45      753    2645\n",
       "reported          266       53     2633     765\n",
       "sars              262       57     2467     931\n",
       "cov               259       60     2319    1079\n",
       "study             257       62     2611     787\n",
       "sars cov          251       68     2180    1218\n",
       "time              251       68     2576     822\n",
       "one               250       69     2317    1081\n",
       "positive          247       72     1892    1506\n",
       "day               247       72     2219    1179\n",
       "may               246       73     2509     889\n",
       "also              246       73     2656     742\n",
       "hospital          244       75     2099    1299\n",
       "however           243       76     2333    1065\n",
       "health            241       78     2660     738\n",
       "acute             241       78     2261    1137"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df.sort_values(by='pres_inc', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T14:55:40.034957Z",
     "start_time": "2020-02-25T14:55:39.987951Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(token_df, open(f'./improved_token_count_df.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
